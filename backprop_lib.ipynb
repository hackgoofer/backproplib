{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two classes: Tensor, DifferentiableOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    data = None\n",
    "    grad = None\n",
    "    requires_grad = True\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        # TODO: add into the grad, not overriding\n",
    "        self.grad = grad\n",
    "        \n",
    "    def update(self, lr=0.1):\n",
    "        self.data = self.data - lr * self.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableOperator:\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "    def backward(self, output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultTensor(Tensor):\n",
    "    # the input operation\n",
    "    # arguments of the input operation\n",
    "    fn = None\n",
    "    args = None\n",
    "    def __init__(self, data, fn, *args):\n",
    "        self.fn = fn\n",
    "        self.args = args\n",
    "        self.data = data\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        super().backward(grad)\n",
    "        self.fn.backward(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixMultiply(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, m1, m2):\n",
    "        tensor = ResultTensor(np.matmul(m1.data, m2.data), self, m1, m2) # np.dot also works\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        # what we have: the output ResultTensor, ie, output.grad...\n",
    "        # what we need to do: populates the gradient of all of the input, ie, m1.grad, m2.grad\n",
    "        m1, m2 = output.args\n",
    "        grad1 = np.matmul(output.grad, m2.data.T)\n",
    "        grad2 = np.matmul(m1.data.T, output.grad)\n",
    "        \n",
    "        m1.backward(grad1)\n",
    "        m2.backward(grad2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23783741 0.43212538 0.28352959]\n",
      " [0.7753874  0.57462035 0.07725642]]\n",
      "[[0.55873305 0.36287997]\n",
      " [0.1699361  0.4447101 ]\n",
      " [0.6790102  0.50643311]]\n",
      "[[0.39884081 0.42206573]\n",
      " [0.5833412  0.57603724]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.rand(2, 3))\n",
    "b = Tensor(np.random.rand(3, 2))\n",
    "print(a.data)\n",
    "print(b.data)\n",
    "\n",
    "c = MatrixMultiply().forward(a, b)\n",
    "print(c.data)\n",
    "c.backward(np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92161303, 0.6146462 , 1.18544332],\n",
       "       [0.92161303, 0.6146462 , 1.18544332]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test on if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9216, 0.6146, 1.1854],\n",
      "        [0.9216, 0.6146, 1.1854]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "b_p = torch.tensor(b.data, requires_grad=True)\n",
    "c_p = torch.mm(a_p, b_p)\n",
    "loss = c_p.sum()\n",
    "loss.backward()\n",
    "print(a_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "* Relu\n",
    "* AddBias\n",
    "* MSE\n",
    " * Make a whole MSE op (Caffe) \n",
    " * Subtraction, Squaring, Meaning (TensorFlow)\n",
    "\n",
    "* CE\n",
    "* Softmax: s(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tensor = ResultTensor(np.where(x.data > 0, x.data, 0), self, x)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        x, = output.args\n",
    "        grad = output.grad * np.where(x.data > 0, 1, 0)\n",
    "        x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtraction(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        tensor = ResultTensor(x1.data-x2.data, self, x1, x2)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        x1, x2 = output.args\n",
    "        grad1 = output.grad\n",
    "        grad2 = -1 * output.grad\n",
    "        \n",
    "        x1.backward(grad1)\n",
    "        x2.backward(grad2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tensor = ResultTensor(x.data**2, self, x)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        x, = output.args\n",
    "        grad = 2 * output.grad * x.data \n",
    "        x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tensor = ResultTensor(np.mean(x.data), self, x)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        x, = output.args\n",
    "        grad = output.grad * 1/x.data.size + np.zeros_like(x.data)\n",
    "        x.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias(DifferentiableOperator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, b):\n",
    "        tensor = ResultTensor(x.data+b.data, self, x, b)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, output):\n",
    "        # Bias does not have the same shape as the x\n",
    "        # Note: not clear to me why the derivative of broadcast is a sum\n",
    "        x, b = output.args\n",
    "        grad_x = output.grad\n",
    "        grad_b = np.sum(output.grad, axis=0)\n",
    "        \n",
    "        x.backward(grad_x)\n",
    "        b.backward(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33786494  0.7045165  -1.27312199]\n",
      " [-0.76477894 -0.7626472   0.11342666]]\n",
      "[[0.33786494 0.7045165  0.        ]\n",
      " [0.         0.         0.11342666]]\n",
      "[[1. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(2, 3))\n",
    "print(a.data)\n",
    "c = Relu().forward(a)\n",
    "print(c.data)\n",
    "c.backward(np.ones((2, 3)))\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "c_p = F.relu(a_p)\n",
    "loss = c_p.sum()\n",
    "loss.backward()\n",
    "print(a_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30427552 -0.66222056 -1.06750844]\n",
      " [ 0.04757668  0.64406412 -0.04436361]]\n",
      "-0.12969604785466024\n",
      "[[0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(2, 3))\n",
    "print(a.data)\n",
    "c = Mean().forward(a)\n",
    "print(c.data)\n",
    "c.backward(1)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "c_p = a_p.mean()\n",
    "loss = c_p.sum()\n",
    "loss.backward()\n",
    "print(a_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test Subtraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44752451  0.99641571  0.0563563 ]\n",
      " [-0.92334486  0.41534708 -0.44654058]]\n",
      "[[ 1.31328856  0.79497774  1.66899498]\n",
      " [-2.10848229  0.41344388  0.2265242 ]]\n",
      "[[0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667]]\n",
      "[[-0.16666667 -0.16666667 -0.16666667]\n",
      " [-0.16666667 -0.16666667 -0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(2, 3))\n",
    "b = Tensor(np.random.randn(2, 3))\n",
    "print(a.data)\n",
    "c = Subtraction().forward(a, b)\n",
    "print(c.data)\n",
    "loss = Mean().forward(c)\n",
    "loss.backward(1)\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667]], dtype=torch.float64)\n",
      "tensor([[-0.1667, -0.1667, -0.1667],\n",
      "        [-0.1667, -0.1667, -0.1667]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "b_p = torch.tensor(b.data, requires_grad=True)\n",
    "c_p = a_p - b_p\n",
    "loss = c_p.mean()\n",
    "loss.backward()\n",
    "print(a_p.grad)\n",
    "print(b_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.72880416  0.44638519 -0.96024022]\n",
      " [ 1.34800874  0.89310222  1.19075777]]\n",
      "[[0.5311555  0.19925973 0.92206128]\n",
      " [1.81712758 0.79763158 1.41790407]]\n",
      "[[-0.24293472  0.14879506 -0.32008007]\n",
      " [ 0.44933625  0.29770074  0.39691926]]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(2, 3))\n",
    "print(a.data)\n",
    "c = Square().forward(a)\n",
    "print(c.data)\n",
    "loss = Mean().forward(c)\n",
    "loss.backward(1)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2429,  0.1488, -0.3201],\n",
      "        [ 0.4493,  0.2977,  0.3969]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "c_p = a_p ** 2\n",
    "loss = c_p.mean()\n",
    "loss.backward()\n",
    "print(a_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test AddBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.39892804  0.11880573 -1.26419115]\n",
      " [-0.97360996 -1.15602519  0.71572582]]\n",
      "[[-0.40932178  2.11966685 -1.12248335]\n",
      " [ 0.0159963   0.84483593  0.85743362]]\n",
      "[[0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667]]\n",
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(np.random.randn(2, 3))\n",
    "b = Tensor(np.random.randn(3))\n",
    "print(a.data)\n",
    "c = AddBias().forward(a, b)\n",
    "print(c.data)\n",
    "loss = Mean().forward(c)\n",
    "loss.backward(1)\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.1667, 0.1667],\n",
      "        [0.1667, 0.1667, 0.1667]], dtype=torch.float64)\n",
      "tensor([0.3333, 0.3333, 0.3333], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a_p = torch.tensor(a.data, requires_grad=True)\n",
    "b_p = torch.tensor(b.data, requires_grad=True)\n",
    "c_p = a_p + b_p\n",
    "loss = c_p.mean()\n",
    "loss.backward()\n",
    "print(a_p.grad)\n",
    "print(b_p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2End Test\n",
    "#### Pytorch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8683, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0124, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0005, grad_fn=<MeanBackward0>)\n",
      "tensor(1.7311e-05, grad_fn=<MeanBackward0>)\n",
      "tensor(5.7242e-07, grad_fn=<MeanBackward0>)\n",
      "tensor(2.0854e-08, grad_fn=<MeanBackward0>)\n",
      "tensor(6.3484e-10, grad_fn=<MeanBackward0>)\n",
      "tensor(2.0531e-11, grad_fn=<MeanBackward0>)\n",
      "tensor(1.0394e-12, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[0.,0],\n",
    "                  [1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 1]])\n",
    "y = torch.tensor([[0], [1], [1], [0]])\n",
    "\n",
    "hidden_size = 5\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X.shape[1], hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_size, y.shape[1]),\n",
    ")\n",
    "\n",
    "sgd = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pred = model(X)\n",
    "    loss = ((pred - y) ** 2).mean()\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    sgd.zero_grad()\n",
    "    loss.backward()\n",
    "    sgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7439, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2501, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# To Demonstrate that XOR cannot be done linearly.\n",
    "\n",
    "X = torch.tensor([[0.,0],\n",
    "                  [1, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 1]])\n",
    "y = torch.tensor([[0], [1], [1], [0]])\n",
    "\n",
    "hidden_size = 5\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X.shape[1], hidden_size),\n",
    "    torch.nn.Linear(hidden_size, y.shape[1]),\n",
    ")\n",
    "\n",
    "sgd = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    pred = model(X)\n",
    "    loss = ((pred - y) ** 2).mean()\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    sgd.zero_grad()\n",
    "    loss.backward()\n",
    "    sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop Lib Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Tensor(np.array([[0.,0], [1, 0], [0, 1], [1, 1]]))\n",
    "y = Tensor(np.array([[0], [1], [1], [0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldLinear(DifferentiableOperator):\n",
    "    def __init__(self, in_size, h_size):\n",
    "        self.in_size = in_size\n",
    "        self.h_size = h_size\n",
    "        self.w = Tensor(np.ones((in_size, h_size)))\n",
    "        self.b = Tensor(np.zeros((1, h_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = MatrixMultiply().forward(x, self.w)\n",
    "        out = AddBias().forward(Tensor(out.data), self.b)\n",
    "        tensor = ResultTensor(out.data, self, x)\n",
    "        return tensor\n",
    "    \n",
    "    def backward(self, out):\n",
    "        x, = out.args\n",
    "        \n",
    "        x_grad = out.grad * self.w.data\n",
    "        w_grad = out.grad * x.data\n",
    "        b_grad = np.sum(out.grad, axis=0)\n",
    "        \n",
    "        x.backward(x_grad)\n",
    "        self.w.backward(w_grad)\n",
    "        self.b.backward(b_grad)\n",
    "#         print(f\"backward called: {w_grad}\\t{b_grad}\")\n",
    "        \n",
    "    def update(self, lr=0.1):\n",
    "        self.w.update(lr)\n",
    "        self.b.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_size, h_size):\n",
    "        self.in_size = in_size\n",
    "        self.h_size = h_size\n",
    "        \n",
    "        self.w = Tensor(np.random.randn(in_size, h_size) * 0.5 + np.ones((in_size, h_size)))\n",
    "#         self.w = Tensor(np.random.randn(in_size, h_size) * 0.05)\n",
    "        self.b = Tensor(np.zeros((1, h_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = MatrixMultiply().forward(x, self.w)\n",
    "        out = AddBias().forward(out, self.b)\n",
    "        return out\n",
    "            \n",
    "    def update(self, lr=0.1):\n",
    "        self.w.update(lr)\n",
    "        self.b.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 26.076136196674582\n",
      "epoch 100: 0.018423169530866517\n",
      "epoch 200: 0.00021248478110615876\n",
      "epoch 300: 1.717420943951188e-06\n",
      "epoch 400: 1.343363600656358e-08\n",
      "epoch 500: 1.0477276053473084e-10\n",
      "epoch 600: 8.177878683891807e-13\n",
      "epoch 700: 6.3804096884287626e-15\n",
      "epoch 800: 4.980032386055118e-17\n",
      "epoch 900: 3.8862877649210135e-19\n",
      "epoch 1000: 3.0321597203244807e-21\n",
      "epoch 1100: 2.363886865090597e-23\n",
      "epoch 1200: 1.843026295901889e-25\n",
      "epoch 1300: 1.4324603052678796e-27\n",
      "epoch 1400: 1.3935430362370784e-29\n",
      "epoch 1500: 1.6589914659983336e-30\n",
      "epoch 1600: 1.1683785692944992e-30\n",
      "epoch 1700: 1.1648025252004656e-30\n",
      "epoch 1800: 1.164802430367919e-30\n",
      "epoch 1900: 1.1648024303654011e-30\n",
      "epoch 2000: 1.1648024303654011e-30\n",
      "epoch 2100: 1.1648024303654011e-30\n",
      "epoch 2200: 1.1648024303654011e-30\n",
      "epoch 2300: 1.1648024303654011e-30\n",
      "epoch 2400: 1.1648024303654011e-30\n",
      "epoch 2500: 1.1648024303654011e-30\n",
      "epoch 2600: 1.1648024303654011e-30\n",
      "epoch 2700: 1.1648024303654011e-30\n",
      "epoch 2800: 1.1648024303654011e-30\n",
      "epoch 2900: 1.1648024303654011e-30\n",
      "epoch 3000: 1.1648024303654011e-30\n",
      "epoch 3100: 1.1648024303654011e-30\n",
      "epoch 3200: 1.1648024303654011e-30\n",
      "epoch 3300: 1.1648024303654011e-30\n",
      "epoch 3400: 1.1648024303654011e-30\n",
      "epoch 3500: 1.1648024303654011e-30\n",
      "epoch 3600: 1.1648024303654011e-30\n",
      "epoch 3700: 1.1648024303654011e-30\n",
      "epoch 3800: 1.1648024303654011e-30\n",
      "epoch 3900: 1.1648024303654011e-30\n",
      "epoch 4000: 1.1648024303654011e-30\n",
      "epoch 4100: 1.1648024303654011e-30\n",
      "epoch 4200: 1.1648024303654011e-30\n",
      "epoch 4300: 1.1648024303654011e-30\n",
      "epoch 4400: 1.1648024303654011e-30\n",
      "epoch 4500: 1.1648024303654011e-30\n",
      "epoch 4600: 1.1648024303654011e-30\n",
      "epoch 4700: 1.1648024303654011e-30\n",
      "epoch 4800: 1.1648024303654011e-30\n",
      "epoch 4900: 1.1648024303654011e-30\n",
      "epoch 5000: 1.1648024303654011e-30\n",
      "epoch 5100: 1.1648024303654011e-30\n",
      "epoch 5200: 1.1648024303654011e-30\n",
      "epoch 5300: 1.1648024303654011e-30\n",
      "epoch 5400: 1.1648024303654011e-30\n",
      "epoch 5500: 1.1648024303654011e-30\n",
      "epoch 5600: 1.1648024303654011e-30\n",
      "epoch 5700: 1.1648024303654011e-30\n",
      "epoch 5800: 1.1648024303654011e-30\n",
      "epoch 5900: 1.1648024303654011e-30\n",
      "epoch 6000: 1.1648024303654011e-30\n",
      "epoch 6100: 1.1648024303654011e-30\n",
      "epoch 6200: 1.1648024303654011e-30\n",
      "epoch 6300: 1.1648024303654011e-30\n",
      "epoch 6400: 1.1648024303654011e-30\n",
      "epoch 6500: 1.1648024303654011e-30\n",
      "epoch 6600: 1.1648024303654011e-30\n",
      "epoch 6700: 1.1648024303654011e-30\n",
      "epoch 6800: 1.1648024303654011e-30\n",
      "epoch 6900: 1.1648024303654011e-30\n",
      "epoch 7000: 1.1648024303654011e-30\n",
      "epoch 7100: 1.1648024303654011e-30\n",
      "epoch 7200: 1.1648024303654011e-30\n",
      "epoch 7300: 1.1648024303654011e-30\n",
      "epoch 7400: 1.1648024303654011e-30\n",
      "epoch 7500: 1.1648024303654011e-30\n",
      "epoch 7600: 1.1648024303654011e-30\n",
      "epoch 7700: 1.1648024303654011e-30\n",
      "epoch 7800: 1.1648024303654011e-30\n",
      "epoch 7900: 1.1648024303654011e-30\n",
      "epoch 8000: 1.1648024303654011e-30\n",
      "epoch 8100: 1.1648024303654011e-30\n",
      "epoch 8200: 1.1648024303654011e-30\n",
      "epoch 8300: 1.1648024303654011e-30\n",
      "epoch 8400: 1.1648024303654011e-30\n",
      "epoch 8500: 1.1648024303654011e-30\n",
      "epoch 8600: 1.1648024303654011e-30\n",
      "epoch 8700: 1.1648024303654011e-30\n",
      "epoch 8800: 1.1648024303654011e-30\n",
      "epoch 8900: 1.1648024303654011e-30\n",
      "epoch 9000: 1.1648024303654011e-30\n",
      "epoch 9100: 1.1648024303654011e-30\n",
      "epoch 9200: 1.1648024303654011e-30\n",
      "epoch 9300: 1.1648024303654011e-30\n",
      "epoch 9400: 1.1648024303654011e-30\n",
      "epoch 9500: 1.1648024303654011e-30\n",
      "epoch 9600: 1.1648024303654011e-30\n",
      "epoch 9700: 1.1648024303654011e-30\n",
      "epoch 9800: 1.1648024303654011e-30\n",
      "epoch 9900: 1.1648024303654011e-30\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "layers = [\n",
    "    Linear(X.data.shape[1], hidden_size),\n",
    "    Linear(hidden_size, y.data.shape[1]),\n",
    "]\n",
    "\n",
    "for epoch in range(10000):\n",
    "    out = layers[0].forward(X)\n",
    "    out = Relu().forward(out)\n",
    "    out = layers[1].forward(out)\n",
    "    \n",
    "    sub = Subtraction().forward(out, y)\n",
    "    sqr = Square().forward(sub)\n",
    "    loss = Mean().forward(sqr)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch}: {loss.data}\")\n",
    "        \n",
    "    loss.backward(1)\n",
    "    layers[0].update()\n",
    "    layers[1].update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
